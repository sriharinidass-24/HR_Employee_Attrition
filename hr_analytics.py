# -*- coding: utf-8 -*-
"""HR_analytics.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1v-e2H7RFnjU6RCScI7-VPRgihf9D7aXd

##1. Import Libraries & Load Data
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report
from imblearn.over_sampling import SMOTE

# Load dataset
df = pd.read_csv("/content/WA_Fn-UseC_-HR-Employee-Attrition.csv")
print("Shape:", df.shape)
df.head()

"""##2. Exploratory Data Analysis (EDA)
2.1 Attrition Count
"""

sns.countplot(data=df, x='Attrition')
plt.title("Overall Attrition Count")
plt.show()

"""2.2 Attrition by Department"""

sns.countplot(data=df, x='Department', hue='Attrition')
plt.title("Attrition by Department")
plt.show()

"""2.3 Attrition by Business Travel"""

sns.countplot(data=df, x='BusinessTravel', hue='Attrition')
plt.title("Attrition by Business Travel")
plt.show()

"""2.4 Attrition by Education Level"""

sns.countplot(data=df, x='Education', hue='Attrition')
plt.title("Attrition by Education Level")
plt.show()

"""2.5 Attrition by Age Group"""

df['AgeGroup'] = pd.cut(df['Age'], bins=[18,25,35,45,60], labels=['18-25','26-35','36-45','46-60'])
sns.countplot(data=df, x='AgeGroup', hue='Attrition')
plt.title("Attrition by Age Group")
plt.show()

"""2.6 Attrition by Years at Company"""

sns.histplot(data=df, x='YearsAtCompany', hue='Attrition', multiple='stack', bins=15)
plt.title("Attrition by Years at Company")
plt.show()

"""2.7 Attrition by Job Satisfaction"""

sns.countplot(data=df, x='JobSatisfaction', hue='Attrition')
plt.title("Attrition by Job Satisfaction (1=Low, 4=High)")
plt.show()

"""##3. Data Preprocessing"""

le = LabelEncoder()
df_encoded = df.copy()

for col in df_encoded.columns:
    if df_encoded[col].dtype == 'object':
        df_encoded[col] = le.fit_transform(df_encoded[col])

X = df_encoded.drop('Attrition', axis=1)
y = df_encoded['Attrition']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# ======================
# CLEAN PREPROCESSING
# ======================

# Make a copy of the dataset
df_encoded = df.copy()

# Drop columns that are used only for EDA (like AgeGroup)
if 'AgeGroup' in df_encoded.columns:
    df_encoded = df_encoded.drop('AgeGroup', axis=1)

# Encode categorical columns into numbers
from sklearn.preprocessing import LabelEncoder
le = LabelEncoder()

for col in df_encoded.columns:
    if df_encoded[col].dtype == 'object':
        df_encoded[col] = le.fit_transform(df_encoded[col])

# Define Features (X) and Target (y)
X = df_encoded.drop('Attrition', axis=1)
y = df_encoded['Attrition']

# Train-Test Split
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

print("X_train shape:", X_train.shape)
print("y_train distribution:\n", y_train.value_counts())

"""##4. Models
4.1 Logistic Regression
"""

log_reg = LogisticRegression(max_iter=1000)
log_reg.fit(X_train, y_train)
y_pred_log = log_reg.predict(X_test)

print("Logistic Regression Accuracy:", accuracy_score(y_test, y_pred_log))
print(confusion_matrix(y_test, y_pred_log))
print(classification_report(y_test, y_pred_log))

"""4.2 Decision Tree"""

dt = DecisionTreeClassifier(max_depth=5, random_state=42, class_weight='balanced')
dt.fit(X_train, y_train)
y_pred_dt = dt.predict(X_test)

print("Decision Tree Accuracy:", accuracy_score(y_test, y_pred_dt))
print(confusion_matrix(y_test, y_pred_dt))
print(classification_report(y_test, y_pred_dt))

importances = pd.Series(dt.feature_importances_, index=X.columns)
importances.nlargest(10).plot(kind='barh')
plt.title("Top 10 Features - Decision Tree")
plt.show()

"""4.3 Logistic Regression + SMOTE"""

smote = SMOTE(random_state=42)
X_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)

log_reg_smote = LogisticRegression(max_iter=1000)
log_reg_smote.fit(X_train_smote, y_train_smote)
y_pred_smote = log_reg_smote.predict(X_test)

print("Logistic Regression (SMOTE) Accuracy:", accuracy_score(y_test, y_pred_smote))
print(confusion_matrix(y_test, y_pred_smote))
print(classification_report(y_test, y_pred_smote))

"""4.4 Random Forest"""

rf = RandomForestClassifier(n_estimators=200, random_state=42, class_weight='balanced')
rf.fit(X_train, y_train)
y_pred_rf = rf.predict(X_test)

print("Random Forest Accuracy:", accuracy_score(y_test, y_pred_rf))
print(confusion_matrix(y_test, y_pred_rf))
print(classification_report(y_test, y_pred_rf))

importances_rf = pd.Series(rf.feature_importances_, index=X.columns)
importances_rf.nlargest(10).plot(kind='barh')
plt.title("Top 10 Features - Random Forest")
plt.show()

"""##5. Summary of Results"""

results = pd.DataFrame({
    "Model": ["Logistic Regression", "Decision Tree", "Logistic + SMOTE", "Random Forest"],
    "Accuracy": [
        accuracy_score(y_test, y_pred_log),
        accuracy_score(y_test, y_pred_dt),
        accuracy_score(y_test, y_pred_smote),
        accuracy_score(y_test, y_pred_rf)
    ],
    "Recall_AttritionYes": [
        classification_report(y_test, y_pred_log, output_dict=True)['1']['recall'],
        classification_report(y_test, y_pred_dt, output_dict=True)['1']['recall'],
        classification_report(y_test, y_pred_smote, output_dict=True)['1']['recall'],
        classification_report(y_test, y_pred_rf, output_dict=True)['1']['recall']
    ]
})
print(results)